{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c4fea5",
   "metadata": {},
   "source": [
    "# Problem statement:\n",
    "Create a Predictive model which can tell weather to approve a loan application or not?\n",
    "Target Variable: Loan_Status\n",
    "Predictors: Gender, Married, Dependents, Education, Self_Employed, ApplicantIncome etc.\n",
    "Loan_Status=\"N\" means the loan was rejected.\n",
    "Loan_Status=\"Y\" means the loan was approved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b6c3b",
   "metadata": {},
   "source": [
    "- Reading the data in python\n",
    "- Identifying the Target variable\n",
    "- Looking at the distribution of Target variable\n",
    "- Basic Data exploration\n",
    "- Rejecting useless columns\n",
    "- Visual Exploratory Data Analysis for data distribution (Histogram and Barcharts)\n",
    "- Feature Selection based on data distribution\n",
    "- Outlier treatment\n",
    "- Missing Values treatment\n",
    "- Visual correlation analysis\n",
    "- Statistical correlation analysis (Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de87aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "LoanData=pd.read_excel('D://Files//loan_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate rows if any\n",
    "LoanData=LoanData.drop_duplicates()\n",
    "LoanData.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d83e591",
   "metadata": {},
   "source": [
    "Looking at the distribution of Target variable\n",
    "If target variable's distribution is too skewed then the predictive modeling will not be possible.\n",
    "Bell curve is desirable but slightly positive skew or negative skew is also fine\n",
    "When performing Classification, make sure there is a balance in the the distribution of each class otherwise it impacts the Machine Learning algorithms ability to learn all the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Creating Bar chart as the Target variable is Categorical\n",
    "GroupedData=LoanData.groupby('Loan_Status').size()\n",
    "GroupedData.plot(kind='bar', figsize=(4,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb62d1d",
   "metadata": {},
   "source": [
    "Basic Data Exploration\n",
    "This step is performed to guage the overall data. The volume of data, the types of columns present in the data. Initial assessment of the data should be done to identify which columns are Quantitative, Categorical or Qualitative.\n",
    "\n",
    "This step helps to start the column rejection process. You must look at each column carefully and ask, does this column affect the values of the Target variable? For example in this case study, you will ask, Does this column affect the approval or rejection of loan? If the answer is a clear \"No\" the remove the column immediately from the data otherwise keep the column for further analysis.\n",
    "\n",
    "In this data \"Loan_ID\" is one such column which is useless to us because it does not affect the approval or rejection of a loan. Hence, we will remove \"Loan_ID\" from the data.\n",
    "\n",
    "There are four commands which are used for Basic data exploration in Python\n",
    "\n",
    "head() : This helps to see a few sample rows of the data\n",
    "info() : This provides the summarized information of the data\n",
    "describe() : This provides the descriptive statistical details of the data\n",
    "nunique(): This helps us to identify if a column is categorical or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcbeab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing the summarized information of data\n",
    "# Data types, Missing values based on number of non-null values Vs total rows etc.\n",
    "# Remove those variables from data which have too many missing values (Missing Values > 30%)\n",
    "# Remove Qualitative variables which cannot be used in Machine Learning\n",
    "LoanData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395e0c4c",
   "metadata": {},
   "source": [
    "### Removing useless variables from data\n",
    "Based on the above report, removing \"Loan_ID\" column from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3bfe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting those columns which are not useful in predictive analysis because these variables are qualitative\n",
    "UselessColumns = ['Loan_ID']\n",
    "LoanData = LoanData.drop(UselessColumns,axis=1)\n",
    "LoanData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f1bc28",
   "metadata": {},
   "source": [
    "#### Visual Exploratory Data Analysis\n",
    "Categorical variables: Bar plot\n",
    "Continuous variables: Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a98d515",
   "metadata": {},
   "source": [
    "Visualize distribution of all the Categorical Predictor variables in the data using bar plots\n",
    "We can spot a categorical variable in the data by looking at the unique values in them. Typically a categorical variable contains less than 20 Unique values AND there is repetition of values, which means the data can be grouped by those unique values.\n",
    "\n",
    "Based on the Basic Data Exploration above, we have spotted eight categorical predictors in the data\n",
    "\n",
    "Categorical Predictors: 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed','Loan_Amount_Term', 'Credit_History', 'Property_Area'\n",
    "\n",
    "We use bar charts to see how the data is distributed for these categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddbc84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting multiple bar charts at once for categorical variables\n",
    "# Since there is no default function which can plot bar charts for multiple columns at once\n",
    "# we are defining our own function for the same\n",
    "\n",
    "def PlotBarCharts(inpData, colsToPlot):\n",
    "    %matplotlib inline\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Generating multiple subplots\n",
    "    fig, subPlot=plt.subplots(nrows=1, ncols=len(colsToPlot), figsize=(40,6))\n",
    "    fig.suptitle('Bar charts of: '+ str(colsToPlot))\n",
    "\n",
    "    for colName, plotNumber in zip(colsToPlot, range(len(colsToPlot))):\n",
    "        inpData.groupby(colName).size().plot(kind='bar',ax=subPlot[plotNumber])\n",
    "\n",
    "#####################################################################\n",
    "# Calling the function\n",
    "PlotBarCharts(inpData=LoanData, colsToPlot=['Gender', 'Married', 'Dependents', 'Education',\n",
    "       'Self_Employed','Loan_Amount_Term', 'Credit_History', 'Property_Area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59ba3b",
   "metadata": {},
   "source": [
    "#### Bar Charts Interpretation\n",
    "These bar charts represent the frequencies of each category in the Y-axis and the category names in the X-axis.\n",
    "\n",
    "The ideal bar chart looks like the chart of \"Property_Area\" column. Where each category has comparable frequency. Hence, there are enough rows for each category in the data for the ML algorithm to learn.\n",
    "\n",
    "If there is a column which shows too skewed distribution like \"Loan_Amount_Term\" where there is only one dominant bar and the other categories are present in very low numbers. These kind of columns may not be very helpful in machine learning. We confirm this in the correlation analysis section and take a final call to select or reject the column.\n",
    "\n",
    "In this data, all the categorical columns except \"Loan_Amount_Term\" have satisfactory distribution for machine learning.\n",
    "\n",
    "Selected Categorical Variables: All the categorical variables are selected.\n",
    "\n",
    "'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed','Loan_Amount_Term', 'Credit_History', 'Property_Area'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088c124d",
   "metadata": {},
   "source": [
    "Visualize distribution of all the Continuous Predictor variables in the data using histograms\n",
    "Based on the Basic Data Exploration, Three continuous predictor variables 'ApplicantIncome', 'CoapplicantIncome',and 'LoanAmount'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms of multiple columns together\n",
    "# Observe that ApplicantIncome and CoapplicantIncome has outliers\n",
    "LoanData.hist(['ApplicantIncome', 'CoapplicantIncome','LoanAmount'], figsize=(18,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b9dad",
   "metadata": {},
   "source": [
    "##### Histogram Interpretation\n",
    "Histograms shows us the data distribution for a single continuous variable.\n",
    "\n",
    "The X-axis shows the range of values and Y-axis represent the number of values in that range. For example, in the above histogram of \"LoanAmount\", there are around 320 rows in data that has a Loan Amount between 90 to 150.\n",
    "\n",
    "The ideal outcome for histogram is a bell curve or slightly skewed bell curve. If there is too much skewness, then outlier treatment should be done and the column should be re-examined, if that also does not solve the problem then only reject the column.\n",
    "\n",
    "Selected Continuous Variables:\n",
    "\n",
    "ApplicantIncome : Selected. Outliers seen beyond 30000, need to treat them.\n",
    "CoapplicantIncome: Selected. Outliers seen beyond 15000, need to treat them.\n",
    "LoanAmount: Selected. Slightly skewed distribution, acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd342b31",
   "metadata": {},
   "source": [
    "#### Outlier treatment\n",
    "Outliers are extreme values in the data which are far away from most of the values. You can see them as the tails in the histogram.\n",
    "\n",
    "Outlier must be treated one column at a time. As the treatment will be slightly different for each column.\n",
    "\n",
    "Why I should treat the outliers?\n",
    "\n",
    "Outliers bias the training of machine learning models. As the algorithm tries to fit the extreme value, it goes away from majority of the data.\n",
    "\n",
    "There are below two options to treat outliers in the data.\n",
    "\n",
    "Option-1: Delete the outlier Records. Only if there are just few rows lost.\n",
    "Option-2: Impute the outlier values with a logical business value\n",
    "Below we are finding out the most logical value to be replaced in place of outliers by looking at the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed61c8",
   "metadata": {},
   "source": [
    "##### Replacing outliers for 'ApplicantIncome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3aa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding nearest values to 30000 mark\n",
    "LoanData['ApplicantIncome'][LoanData['ApplicantIncome']>20000].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a73d5ab",
   "metadata": {},
   "source": [
    "Above result shows the nearest logical value is 23803, hence, replacing any value above 30000 with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing outliers with nearest possibe value\n",
    "LoanData['ApplicantIncome'][LoanData['ApplicantIncome']>30000] = 23803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Replacing outliers for 'CoapplicantIncome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding nearest values to 15000 mark\n",
    "LoanData['CoapplicantIncome'][LoanData['CoapplicantIncome']>10000].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f92013",
   "metadata": {},
   "source": [
    "Above result shows the nearest logical value is 11300, hence, replacing any value above 15000 with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d6e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing outliers with nearest possibe value\n",
    "LoanData['CoapplicantIncome'][LoanData['CoapplicantIncome']>15000] = 11300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e499fb31",
   "metadata": {},
   "source": [
    "##### Visualizing distribution after outlier treatment\n",
    "The distribution has improved after the outlier treatment. There is still a tail but it is thick, that means there are many values in that range, hence, it is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoanData.hist(['ApplicantIncome', 'CoapplicantIncome'], figsize=(18,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc5436",
   "metadata": {},
   "source": [
    "#### Missing values treatment\n",
    "Missing values are treated for each column separately.\n",
    "\n",
    "If a column has more than 30% data missing, then missing value treatment cannot be done. That column must be rejected because too much information is missing.\n",
    "\n",
    "There are below options for treating missing values in data.\n",
    "\n",
    "Delete the missing value rows if there are only few records\n",
    "Impute the missing values with MEDIAN value for continuous variables\n",
    "Impute the missing values with MODE value for categorical variables\n",
    "Interpolate the values based on nearby values\n",
    "Interpolate the values based on business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b35856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding how many missing values are there for each column\n",
    "LoanData.isnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a6e133a",
   "metadata": {},
   "source": [
    "Here, I am choosing to impute missing values using Median and Mode values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing the missing values\n",
    "# Using MODE for categorical columns\n",
    "LoanData['Gender'].fillna(LoanData['Gender'].mode()[0], inplace=True)\n",
    "LoanData['Married'].fillna(LoanData['Married'].mode()[0], inplace=True)\n",
    "LoanData['Dependents'].fillna(LoanData['Dependents'].mode()[0], inplace=True)\n",
    "LoanData['Self_Employed'].fillna(LoanData['Self_Employed'].mode()[0], inplace=True)\n",
    "# Using Mode value for Loan_Amount_Term since it is a categorical variable\n",
    "LoanData['Loan_Amount_Term'].fillna(LoanData['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "LoanData['Credit_History'].fillna(LoanData['Credit_History'].mode()[0], inplace=True)\n",
    "\n",
    "# Using Median value for continuous columns\n",
    "LoanData['LoanAmount'].fillna(LoanData['LoanAmount'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values again after the treatment\n",
    "LoanData.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebab63",
   "metadata": {},
   "source": [
    "Feature Selection\n",
    "Now its time to finally choose the best columns(Features) which are correlated to the Target variable. This can be done directly by measuring the correlation values or ANOVA/Chi-Square tests. However, it is always helpful to visualize the relation between the Target variable and each of the predictors to get a better sense of data.\n",
    "\n",
    "I have listed below the techniques used for visualizing relationship between two variables as well as measuring the strength statistically.\n",
    "\n",
    "Visual exploration of relationship between variables\n",
    "Continuous Vs Continuous ---- Scatter Plot\n",
    "Categorical Vs Continuous---- Box Plot\n",
    "Categorical Vs Categorical---- Grouped Bar Plots\n",
    "Statistical measurement of relationship strength between variables\n",
    "Continuous Vs Continuous ---- Correlation matrix\n",
    "Categorical Vs Continuous---- ANOVA test\n",
    "Categorical Vs Categorical--- Chi-Square test\n",
    "In this case study the Target variable is categorical, hence below two scenarios will be present\n",
    "\n",
    "Categorical Target Variable Vs Continuous Predictor\n",
    "Categorical Target Variable Vs Categorical Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533cc7e",
   "metadata": {},
   "source": [
    "#### Relationship exploration: Categorical Vs Continuous -- Box Plots\n",
    "When the target variable is Categorical and the predictor variable is Continuous we analyze the relation using bar plots/Boxplots and measure the strength of relation using Anova test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for Categorical Target Variable \"Loan_Status\" and continuous predictors\n",
    "ContinuousColsList=['ApplicantIncome','CoapplicantIncome', 'LoanAmount']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, PlotCanvas=plt.subplots(nrows=1, ncols=len(ContinuousColsList), figsize=(18,5))\n",
    "\n",
    "# Creating box plots for each continuous predictor against the Target Variable \"Loan_Status\"\n",
    "for PredictorCol , i in zip(ContinuousColsList, range(len(ContinuousColsList))):\n",
    "    LoanData.boxplot(column=PredictorCol, by='Loan_Status', figsize=(5,5), vert=True, ax=PlotCanvas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c53ea2b",
   "metadata": {},
   "source": [
    "Box-Plots interpretation\n",
    "What should you look for in these box plots?\n",
    "\n",
    "These plots gives an idea about the data distribution of continuous predictor in the Y-axis for each of the category in the X-Axis.\n",
    "\n",
    "If the distribution looks similar for each category(Boxes are in the same line), that means the the continuous variable has NO effect on the target variable. Hence, the variables are not correlated to each other.\n",
    "\n",
    "For example, look at the first chart \"ApplicantIncome\" Vs \"Loan_Status\". The boxes are in the same line! It means that people whose loan was rejected and whose loan was approved have same kind of incomes. Hence, I cannot distinguish between approval and rejection based on the income of an applicant. So this column is NOT correlated with the Loan_Status.\n",
    "\n",
    "The other other two charts also exhibit same characteristics, hence all three continuous predictors are not correlated with the target variable.\n",
    "\n",
    "We confirm this by looking at the results of ANOVA test below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e5db1e",
   "metadata": {},
   "source": [
    "Statistical Feature Selection (Categorical Vs Continuous) using ANOVA test\n",
    "Analysis of variance(ANOVA) is performed to check if there is any relationship between the given continuous and categorical variable\n",
    "\n",
    "Assumption(H0): There is NO relation between the given variables (i.e. The average(mean) values of the numeric Predictor variable is same for all the groups in the categorical Target variable)\n",
    "ANOVA Test result: Probability of H0 being true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to find the statistical relationship with all the categorical variables\n",
    "def FunctionAnova(inpData, TargetVariable, ContinuousPredictorList):\n",
    "    from scipy.stats import f_oneway\n",
    "\n",
    "    # Creating an empty list of final selected predictors\n",
    "    SelectedPredictors=[]\n",
    "    \n",
    "    print('##### ANOVA Results ##### \\n')\n",
    "    for predictor in ContinuousPredictorList:\n",
    "        CategoryGroupLists=inpData.groupby(TargetVariable)[predictor].apply(list)\n",
    "        AnovaResults = f_oneway(*CategoryGroupLists)\n",
    "        \n",
    "        # If the ANOVA P-Value is <0.05, that means we reject H0\n",
    "        if (AnovaResults[1] < 0.05):\n",
    "            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n",
    "            SelectedPredictors.append(predictor)\n",
    "        else:\n",
    "            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n",
    "    \n",
    "    return(SelectedPredictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function to check which categorical variables are correlated with target\n",
    "ContinuousVariables=['ApplicantIncome', 'CoapplicantIncome','LoanAmount']\n",
    "FunctionAnova(inpData=LoanData, TargetVariable='Loan_Status', ContinuousPredictorList=ContinuousVariables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a261a2c",
   "metadata": {},
   "source": [
    "The results of ANOVA confirm our visual analysis using box plots above.\n",
    "\n",
    "All three columns are NOT correlated with Loan_Status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170886c",
   "metadata": {},
   "source": [
    "#### Relationship exploration: Categorical Vs Categorical -- Grouped Bar Charts\n",
    "When the target variable is Categorical and the predictor is also Categorical then we explore the correlation between them visually using barplots and statistically using Chi-square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c263f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross tablulation between two categorical variables\n",
    "CrossTabResult=pd.crosstab(index=LoanData['Gender'], columns=LoanData['Loan_Status'])\n",
    "CrossTabResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915afd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Inference using Grouped Bar charts\n",
    "CategoricalColsList=['Gender', 'Married', 'Dependents', 'Education',\n",
    "       'Self_Employed','Loan_Amount_Term', 'Credit_History', 'Property_Area']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, PlotCanvas=plt.subplots(nrows=len(CategoricalColsList), ncols=1, figsize=(10,50))\n",
    "\n",
    "# Creating Grouped bar plots for each categorical predictor against the Target Variable \"Loan_Status\"\n",
    "for CategoricalCol , i in zip(CategoricalColsList, range(len(CategoricalColsList))):\n",
    "    CrossTabResult=pd.crosstab(index=LoanData[CategoricalCol], columns=LoanData['Loan_Status'])\n",
    "    CrossTabResult.plot.bar(color=['red','blue'], ax=PlotCanvas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff82a0b",
   "metadata": {},
   "source": [
    "#### Grouped Bar charts Interpretation\n",
    "What to look for in these grouped bar charts?\n",
    "\n",
    "These grouped bar charts show the frequency in the Y-Axis and the category in the X-Axis. If the ratio of bars is similar across all categories, then the two columns are not correlated. For example, look at the Gender Vs Loan_Status plot. The approved vs rejected ratio for Males is similar to Females, it means Gender does not affect the Loan approval!. Hence, these two variables are not correlated.\n",
    "\n",
    "On the other hand, look at the Credit_History vs Loan_Status plot. The number of approvals are very high if Credit_History=1.0. It means Credit_History affects the loan approval! Hence, two columns are correlated with each other.\n",
    "\n",
    "We confirm this analysis in below section by using Chi-Square Tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56114f6f",
   "metadata": {},
   "source": [
    "Statistical Feature Selection (Categorical Vs Categorical) using Chi-Square Test\n",
    "Chi-Square test is conducted to check the correlation between two categorical variables\n",
    "\n",
    "Assumption(H0): The two columns are NOT related to each other\n",
    "Result of Chi-Sq Test: The Probability of H0 being True\n",
    "More information on ChiSq: https://www.mathsisfun.com/data/chi-square-test.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e480e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a function to find the correlation of all categorical variables with the Target variable\n",
    "def FunctionChisq(inpData, TargetVariable, CategoricalVariablesList):\n",
    "    from scipy.stats import chi2_contingency\n",
    "    \n",
    "    # Creating an empty list of final selected predictors\n",
    "    SelectedPredictors=[]\n",
    "\n",
    "    for predictor in CategoricalVariablesList:\n",
    "        CrossTabResult=pd.crosstab(index=inpData[TargetVariable], columns=inpData[predictor])\n",
    "        ChiSqResult = chi2_contingency(CrossTabResult)\n",
    "        \n",
    "        # If the ChiSq P-Value is <0.05, that means we reject H0\n",
    "        if (ChiSqResult[1] < 0.05):\n",
    "            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', ChiSqResult[1])\n",
    "            SelectedPredictors.append(predictor)\n",
    "        else:\n",
    "            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', ChiSqResult[1])        \n",
    "            \n",
    "    return(SelectedPredictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "CategoricalVariables=['Gender', 'Married', 'Dependents', 'Education',\n",
    "       'Self_Employed','Loan_Amount_Term', 'Credit_History', 'Property_Area']\n",
    "\n",
    "# Calling the function\n",
    "FunctionChisq(inpData=LoanData, \n",
    "              TargetVariable='Loan_Status',\n",
    "              CategoricalVariablesList= CategoricalVariables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41185bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(LoanData.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c728af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Covariance\n",
    "LoanData.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix=LoanData.cov()\n",
    "sns.heatmap(cov_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d53a0",
   "metadata": {},
   "source": [
    "We did Exploratory data Analysis on the features of this dataset and saw how each feature is\n",
    "distributed. We also calculated probabilities to prove an association among the Independent\n",
    "variables and the Target variable. And based on the results, we assumed whether or not there\n",
    "is an association.\n",
    "Lastly, Statistical Tests were conducted so as to confirm or deny the assumptions we made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoanData.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3349856c",
   "metadata": {},
   "source": [
    "# Model Building -Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the features\n",
    "selected_columns=['Married','Education','Credit_History','Property_Area']\n",
    "Data_log=LoanData[selected_columns]\n",
    "Data_log.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c915ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical into numeric\n",
    "Data_log=pd.get_dummies(Data_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_log['Loan_Status']=LoanData['Loan_Status']\n",
    "Data_log.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695eac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X=Data_log.drop('Loan_Status',axis=1)\n",
    "y=Data_log['Loan_Status']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "predictorscalar=MinMaxScaler()\n",
    "predictorscalarFit=predictorscalar.fit(X)\n",
    "X=predictorscalarFit.transform(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression()\n",
    "LOG= clf.fit(X_train,y_train)\n",
    "#prediction\n",
    "prediction=LOG.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4bb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be1f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure accuracy\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test,prediction))\n",
    "print(metrics.confusion_matrix(y_test,prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing overall accuracy\n",
    "metrics.f1_score(y_test,prediction,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "LOG.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402adb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d79afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_var(x):\n",
    "    if x=='Y':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "y_test=y_test.map(new_var)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9339429",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca2b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=LOG.predict_proba(X_test)\n",
    "#class 1 probabilities\n",
    "pred_prob=y_pred[:,1]\n",
    "pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2417a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "fpr,tpr,threshold=roc_curve(y_test, pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tpr)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db8cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34fc42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score1=roc_auc_score(y_test,pred_prob)\n",
    "roc_auc_score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b94824",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot ROCR-AUC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(fpr,tpr)\n",
    "plt.plot(fpr, tpr,label= 'Logistic Regression (sensitivity=%0.2f)' %roc_auc_score1)\n",
    "plt.plot([0, 0],[1, 0] , c=\".7\")\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.xlim(0.0 ,1.0)\n",
    "plt.ylim(0.0,1.05)\n",
    "plt.title('Receiver Operating Characteristic' )\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f007394",
   "metadata": {},
   "source": [
    "#### Finding significant variables using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X=Data_log.drop('Loan_Status',axis=1)\n",
    "y=Data_log[['Loan_Status']]\n",
    "def new_var(x):\n",
    "    if x=='Y':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "y['Loan_Status']=y['Loan_Status'].map(new_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b118b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100)\n",
    "#logreg=sm.Logit(X_train, y_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b90187",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg=sm.Logit(y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73322559",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effeee24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
